{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Implement a crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der **Crawler** wurde mit Hilfe der Bibliothek 'scrapy' geschrieben. Die dazu implementierte Anwendung ist im Verzeichnis ~/sitespider zu finden und der eigentliche Crawler in der Datei ~/sitespider/sitespider/spiders/site_spider.py. Der Code dazu sieht wie folgt aus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class SiteSpiderSpider(scrapy.Spider):\n",
    "    name = 'site_spider'\n",
    "\n",
    "    start_urls = [\n",
    "        'http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d01.html',\n",
    "        'http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d06.html',\n",
    "        'http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d08.html'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        id = response.url.split('/')[-1].split('.')[0]\n",
    "\n",
    "        with open(\"sites/%s.json\" % id, 'w') as f:\n",
    "            data = {'id': id,\n",
    "                    'text': \"\".join(line for line in response.css('body::text').extract()),\n",
    "                    'url': response.url,\n",
    "                    'back_links': [link.split('.')[0] for link in response.css('a::attr(href)').extract()]\n",
    "                    }\n",
    "\n",
    "            json.dump(data, f)\n",
    "\n",
    "        for next_page in response.css('a::attr(href)').extract():\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse **SiteSpiderSpider** erbt von der Klasse scrapy.Spider. So kann die geschriebene Klasse später als Crawler benutzen werden.\n",
    "\n",
    "Das Attribut **name** identifizieren den Crwaler und wird dazu benutzt, den Crawler später über die Konsole aufzurufen. Das Attribut **start_urls** legt fest, welche URLs vom Crawler initial angefragt werden sollen. Die Methode **parse()** nimmt dann anschließend die Rückgabe jeder Anfrage entgegen und bearbeitet diese. Da die Methode parse() die default Callback-Methode ist, muss diese nicht explizit angegeben werden.\n",
    " \n",
    "Die **parse()-Methode** erzeugt einen String, der die Anfrage eindeutig beschreibt. Dieser Wert wird anschließend als Dateiname für die Datei verwendet, in der die Daten der Anfrage gespeichert werden. Gewähltes Fromat ist hierbei JSON. In der Datei selber wird die Id der Anfrage, der eigentliche Text ohne HTML-Tags, die URL und alle auf der Seite gefundenen Links als Array gespeichert. Anschließend wird für alle auf der Seite gefundenen Links derselbe Prozess mittels der Methode response.follow() angestoßen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Crawl all reachable documents starting from the following URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der implementierte **Crawler** kann mittels des Konsolen-Befehls 'scrapy crawl site_spider' in dem Verzeichniss ~/sitespider aufgerufen werden. Die resultierenden Dateien sind in dem Unterordner ~/sitespider/sites/\\* zu finden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Calculate the PageRanks of the downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgend werden die **PageRanks** der heruntergeladenen Seiten berechnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende **Variablen** werden verwendet:\n",
    "* **directory**: Ordner, in dem die resultierenden Dateien liegen\n",
    "* **files**: resultierende Dateien\n",
    "* **pages**: Dictionary-Array, wobei jedes Dictionary jeweils eine Seite repräsentiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = 'sitespider/sites'\n",
    "files = [x[2] for x in os.walk(directory)][0]\n",
    "pages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importierung der analysierten Seiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Code-Block werden die resultierenden Dateien einzeln ausgelesen und in der Variablen pages gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(\"%s/%s\" % (directory, file)) as json_data:\n",
    "        pages += [json.load(json_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstanten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgenden **Konstenten** sind vorgegeben bzw. müssen zuvor berechnet werden:\n",
    "* **n** = Anzahl der betrachteten Seiten\n",
    "* **t** = Teleportationsrate\n",
    "* **d** = Dämpfungsfaktor\n",
    "* **δ** = Abbruchswert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(pages)\n",
    "t = 0.05\n",
    "d =  1 - t\n",
    "δ = 0.04 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PageRank-Funktion**\n",
    "\n",
    "Die Funktion **calculate_page_rank()** berechnet den PageRank für die übergebene Seite page_i. Folgende Formel wurde dabei implementiert:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ r_{k+1}(p_i) = d * \\bigg ( \\displaystyle\\sum_{p_j \\in B_{pi}} \\frac{r_k(p_j)}{|p_j|} + \\displaystyle\\sum_{p_j,|p_j|=0} \\frac{r_k(p_j)}{N} \\bigg ) + \\frac{t}{N} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_page_rank(page_i):\n",
    "    sum_result = 0  \n",
    "    for page_j in pages:\n",
    "        if page_i['id'] in page_j['back_links']:\n",
    "            sum_result += page_j['rank'] / len(page_j['back_links'])\n",
    "        if len(page_j['back_links']) == 0:\n",
    "            sum_result += page_j['rank'] / n\n",
    "    return d * sum_result + t / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hilfe-Funktionen**\n",
    "\n",
    "Die Funktion **initialize()** initialisiert alle Seiten mit einem Start-PageRank, der sich durch die Division von eins durch die Anzahl aller Seiten berechnet. Zudem setzt sie den folgenden PageRank auf None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    for page in pages:\n",
    "        page['rank'] = 1/n\n",
    "        page['rank+1'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion **termination_condition()** gibt einen boolischen Wert zurück, der sagt, ob die Abbruchbedingung erfüllt ist oder nicht. Im Falle, dass es keine zwei Vergleichswerte gibt, also rank+1 gleich None ist, wird False zurückgegeben, ansonsten wird der Rückgabewert mit folgender Formel berechnet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sum_{p_i,i \\in 1,...,N} abs(r_{k+1}(p_i) - r_k(p_i)) <= \\delta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termination_condition():\n",
    "    delta = 0\n",
    "    for page in pages:\n",
    "        if page['rank+1'] is None:\n",
    "            return False\n",
    "        else:\n",
    "            delta += abs(page['rank+1'] - page['rank'])\n",
    "    return  delta <= δ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion **set_ranks()** setzt, wenn rank+1 nicht None ist, für alle Seiten den älteren PageRank-Wert rank auf den aktuellen PageRank-Wert rank+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_ranks():\n",
    "    for page in pages:\n",
    "        if  page['rank+1'] is not None:\n",
    "            page['rank'] = page['rank+1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung der PageRanks für alle Dokumente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Code-Block wird zunächst die Methode initialize() aufgerufen, um alle Seiten mit einem initalen PageRank-Wert zu initialisieren. Anschließend wird so lange die While-Schleife durchlaufen, bis die Abbruchsbedingung erfüllt ist. Innerhalb der While-Schleife wird zunächst die Methode set_ranks() aufgerufen, um alle alten PageRanks auf den aktuellen Stand zu setzen. Anschließend wird jede Seite durchlaufen, wobei jeweils der nächste PageRank für die aktuelle Seite berechnet wird und in der Variablen rank+1 gespeichert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initialize()\n",
    "while not termination_condition():\n",
    "    set_ranks()\n",
    "    for page in pages:\n",
    "        page['rank+1'] = calculate_page_rank(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichern der PageRanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **Resultate** werden in der Datei rank.txt gespeichert. Dabei steht in der Datei zunächst die Id der Seite und anschließend der eigentliche PageRank-Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('rank.txt', 'w') as f:\n",
    "    for page in pages:\n",
    "        f.write(\"%s: %s\\n\" % (page['id'], page['rank+1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Überprüfung der Summe aller PageRanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Abschluß wird die **Summe aller PageRanks** berechnet, um zu prüfen, ob dieser Wert ungefähr 1 entspricht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "rank_sum = 0\n",
    "for page in pages:\n",
    "    rank_sum += page['rank+1']\n",
    "print(rank_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Build a tf-Index for the words contained in the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Abschnitt wird ein **TF-Index** für alle Wörter, die in den Dokumenten (Seiten) auftauchen, berechnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende **Variablen** werden verwendet:\n",
    "* **tf_dict**: Dictionary, das den TF-Index repräsentiert\n",
    "* **term_set**: Set, bestehend aus allen vorkommenden Begriffen innerhalb der Dokumente \n",
    "* **stopwords**: Liste von Stopwörtern\n",
    "* **exclude**: Set von Zeichen, die entfernt werden sollen\n",
    "* **porter**: Stemmer, der zum Finden der Wortstämme verwendet werden soll; in dem Fall der nltk-Porter-Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_dict = {}\n",
    "term_set = set()\n",
    "stopwords = []\n",
    "exclude = set(string.punctuation)\n",
    "porter = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gewichtete Term-Frequency**\n",
    "\n",
    "Die Funktion **get_weighted_tf()** brechnet mittels der folgenden Formel die gewichtete Term-Frequency für den übergebenen Term im Bezug auf das Dokument, mit der übergebenen Dokumenten-Id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{t,d} =\n",
    "  \\begin{cases}\n",
    "    1 + log_{10}(tf_{t,d})       & \\quad \\text{if } tf_{t,d} > 0\\\\\n",
    "    0 & \\quad \\text{otherwise}\\\\\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_tf(doc_id, term):\n",
    "    if term in tf_dict[doc_id]:\n",
    "        if tf_dict[doc_id][term] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 + math.log10(tf_dict[doc_id][term])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalisierung der Stopwörter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Code-Block werden die **Stopwörter** aus der Datei stop_words.txt ausgelesen und in der Variablen stopwords gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('stop_words.txt') as line:\n",
    "    stopwords += re.sub('[^a-zA-Z0-9,]', '', line.read()).split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung aller vorkommenden Wörter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um alle vorkommenden Wörter zu berechnen, werden alle Texte der Dokumente durchlaufen und mittels des nltk-Wörter-Tokinisierers tokinisiert. Jedes resultierende Wort wird anschließend analysiert und sofern es kein Zeichen oder ein Stopwort ist, dem term_set hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            term_set.add(porter.stem(term).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung des Term-Frequency-Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Term-Frequency-Indexes aufzustellen, wird jedes Dokument durchlaufen, wobei zunächst für jedes Wort aus dem term_set ein Wert von 0 initialisiert wird. Anschließend wird jedes Wort innherhalb des Dokumentes analysiert und, sofern es sich um kein Zeichen oder Stopwort handelt, der passende Zähler um eins erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    tf_dict[page['id']] = {}\n",
    "    for term in term_set:\n",
    "        tf_dict[page['id']][term] = 0\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            tf_dict[page['id']][porter.stem(term).lower()] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichern des Term-Frequency-Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das **Resultat** wird in der Datei index.txt gespeichert. Dabei stellt jede Zeile ein Wort und jede Spalte ein Dokument dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(tf_dict)\n",
    "tf_df.to_csv('index.txt', header=True, index=True, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a function search to search for documents containing given words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgend wird ein **TF-Index** für alle Wörter innerhalb der Dokumente (Seiten) berechnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variablen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende **Variablen** werden verwendet:\n",
    "* **df_dict**: Dictionary, das für jedes Wort die Anzahl festhält, in wie vielen Dokumenten das Wort vorkommt und die Dokumenten, in denen es vorkommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gewichtete Inverse-Document-Frequency**\n",
    "\n",
    "Die Funktion **get_weighted_idf()** berechnet mittels der folgenden Formel die gewichtete Inverse-Document-Frequency für den übergebenen Term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_t =\n",
    "  \\begin{cases}\n",
    "    log_{10}( \\frac{N}{df_t})       & \\quad \\text{if } df_t > 0\\\\\n",
    "    0 & \\quad \\text{otherwise}\\\\\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_idf(term):\n",
    "    if term in df_dict:\n",
    "        if df_dict[term]['count'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.log10( n / df_dict[term]['count'])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gewichteter TF-IDF-Wert**\n",
    "\n",
    "Die Funktion **get_weighted_tf_idf()** brechnet mittels der folgenden Formel den gewichteten TF-IDF-Wert für den übergebenen Term im Bezug auf das Dokument mit der übergebenen Dokumenten-Id.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{tf-idf}_{t,d} = tf_{t,d} \\times idf_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_tf_idf(doc_id, term):\n",
    "    return get_weighted_tf(doc_id, term) * get_weighted_idf(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search**\n",
    "\n",
    "Die Funktion **search()** brechnet für jede Seite einen Wert, der aussagt, wie gut die jeweilge Seite zu den gesuchten Termen passt. Dabei berechnet sich der Wert **ohne Einbeziehung des PageRanks** wie folgt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Score}(q, d) = \\sum_{t \\in q} \\text{tf-idf}_{t,d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit Einbeziehung des PageRanks** berechnet sich der Wert wie folgt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Score}(q, d) = \\sum_{t \\in q} \\text{tf-idf}_{t,d} \\times PageRank_d $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(terms, page_rank=False):\n",
    "    result = {}\n",
    "    for page in pages:\n",
    "        result[page['id']] = 0\n",
    "        for term in terms:\n",
    "            result[page['id']] += get_weighted_tf_idf(page['id'], porter.stem(term).lower())\n",
    "            if page_rank:\n",
    "                result[page['id']] *= page['rank+1']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berechnung der Document-Frequency für jedes Wort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Berechnung der Document-Frequency werden alle Seiten durchlaufen, wobei jedes Wort des Textes der Seite betrachtet wird. Sofern das Wort kein Zeichen oder Stopwort ist und noch nicht im df_dict vorkommt, wird es neu mit der  Document-Frequency 1 und der Dokumenten-Id angelegt. Existiert das Wort bereits im df_dict, wird nur dann die Document-Frequency um eins erhöht, wenn die aktuelle Dokumenten-Id noch nicht existiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            if porter.stem(term).lower() in df_dict and page['id'] not in df_dict[porter.stem(term).lower()]['documents']:\n",
    "                df_dict[porter.stem(term).lower()]['count'] += 1\n",
    "                df_dict[porter.stem(term).lower()]['documents'] += [page['id']]\n",
    "            elif porter.stem(term).lower() not in df_dict:\n",
    "                df_dict[porter.stem(term).lower()] = {'count': 1,\n",
    "                                                      'documents': [page['id']]\n",
    "                                                     }          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche mit TF-IDF und anschließende Speicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gesucht** wird nach folgenden **Termen**:\n",
    "* **token**\n",
    "* **index**\n",
    "* **classification**\n",
    "* **classification, token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_terms = [['token'],['index'],['classification'],['classification', 'token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Code-Block wird jeder **Such-Term** durchgegangen und der jeweilige Wert für jedes Dokument berechnet. Der Wert setzt sich aus dem **TF-IDF-Wert** zusammen. Anschließend wird das **Ergebnis** in der Datei pageranke_search.txt gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tfidf_search.txt', 'w') as f:\n",
    "    for search_term in search_terms:\n",
    "        f.write('Suchwort: %s\\n\\n' % ', '.join(search_term))\n",
    "        result = search(search_term)\n",
    "        for key in result:\n",
    "            f.write(\"%s: %s\\n\" % (key, result[key]))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## f) Extend your search function and include PageRank to score the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suche mit TF-IDF und PageRank und anschließende Speicherung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gesucht** wird nach folgenden **Termen**:\n",
    "* **token**\n",
    "* **index**\n",
    "* **classification**\n",
    "* **classification, token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_terms = [['token'],['index'],['classification'],['classification', 'token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Code-Block wird jeder **Such-Term** durchgegangen und der jeweilige Wert für jede Seite berechnet. Der Wert setzt sich aus dem **TF-IDF-Wert und PageRank** zusammen. Anschließend wird das **Ergebnis** in der Datei pageranke_search.txt gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pageranke_search.txt', 'w') as f:\n",
    "    for search_term in search_terms:\n",
    "        f.write('Suchwort: %s\\n\\n' % ', '.join(search_term))\n",
    "        result = search(search_term, page_rank=True)\n",
    "        for key in result:\n",
    "            f.write(\"%s: %s\\n\" % (key, result[key]))\n",
    "        f.write('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
