{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Fragen ob bei suche nach mehreren Wörtern einfach summe gebildet werden kann?\n",
    "* Daten der Suche abspeichern\n",
    "* Doku\n",
    "* Werden bei Werte bei PageRank und TFIDF Suche addiert oder mulitpliziert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Implement a crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Crawler wurde mit hielfe der Bibliothek scrapy geschrieben. Der eigen implementierte Code ist in der Datei ~/sitespider/sitespider/spiders/site_spider.py zu finden und sieht wie folgt aus:\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class SiteSpiderSpider(scrapy.Spider):\n",
    "    name = 'site_spider'\n",
    "\n",
    "    start_urls = [\n",
    "        'http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d01.html',\n",
    "        'http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d06.html',\n",
    "        'http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d08.html'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        id = response.url.split('/')[-1].split('.')[0]\n",
    "\n",
    "        with open(\"sites/%s.json\" % id, 'w') as f:\n",
    "            data = {'id': id,\n",
    "                    'text': \"\".join(line for line in response.css('body::text').extract()),\n",
    "                    'url': response.url,\n",
    "                    'back_links': [link.split('.')[0] for link in response.css('a::attr(href)').extract()]\n",
    "                    }\n",
    "\n",
    "            json.dump(data, f)\n",
    "\n",
    "        for next_page in response.css('a::attr(href)').extract():\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Die Klasse SiteSpiderSpider erbt von der Klasse scrapy.Spider um später, die geschriebene Klasse mittels der scrapy Bibliothek als Crawler einsetzen zu könnnen.\n",
    "\n",
    "Das Attribut \"name\" identifizieren den Crwaler und wird dazu benutzt, den Crawler über die Konsole aufzurufen. Das Attribut \"start_urls\" legt fest, welche URL's vom Crawler initial angefragt werden sollen. Die Methode parse() nimmt dann anschlißend die Rückgabe jeder dieser Anfrage entgegen und behandelt diese. Da die Methode parse() die default callback methode von scrapy ist, muss dies nicht explezit angegeben werden.\n",
    " \n",
    "Die \"parse()\"-Methode erzeugt ein String, der die Anfrage eindeutig beschriebt. Dieser Wert wir in der Variablen \"id\" gespeichert und anschließend als Dateiname verwendet, in dem die Daten im JSON-Format der Anfrage gespeichert werden. In der Datei selber wird die Id, der eigentliche Text, der angefragten Seite ohne HTML-Tags, die URL, und alle auf der Seite gefundenen Links als Array, gespeichert. Anschließend wird für alle auf der Seite gefundenen Links der selbe Prozess mittels der Methode \"response.follow(..)\" angestoßen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Crawl all reachable documents starting from the following URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der implementierte Crawler wird mittels des Konsolen-Befehls 'scrapy crawl site_spider' in dem Verzeichniss ~/sitespider aufgerufen. Die resultierenden Dateien sind in dem Unterordner ~/sitespider/sites/\\* zufinden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing crawled documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Code-Block werden die Dateien mit den Daten der gecrawlten Seiten Datei für Datei ausgelesen und in der Variablen pages gespechert. Die Variable pages ist ein DictionaryArray, wobei jedes Dictionaty eine Seite representiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'sitespider/sites'\n",
    "files = [x[2] for x in os.walk(directory)][0]\n",
    "pages = []\n",
    "\n",
    "for file in files:\n",
    "    with open(\"%s/%s\" % (directory, file)) as json_data:\n",
    "        pages += [json.load(json_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Calculate the PageRanks of the downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden die PageRanks der Heruntergeladenen Seiten berechnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstanten\n",
    "\n",
    "Die folgenden Konstenten beschereiben folgendes:\n",
    "* n = Anzahl der betrachteten Seiten\n",
    "* t = Teleportationsrate\n",
    "* d = Dämpfungsfaktor\n",
    "* δ = Abbruchswert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(pages)\n",
    "t = 0.05\n",
    "d =  1 - t\n",
    "δ = 0.04 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PageRank**\n",
    "\n",
    "Die Funktion calculate_page_rank() berechnet den PageRank für die übergebene Seite page_i. Folgende Formel wurde dabei implementiert:\n",
    "\n",
    "$$ r_{k+1}(p_i) = d * \\bigg ( \\displaystyle\\sum_{p_j \\in B_{pi}} \\frac{r_k(p_j)}{|p_j|} + \\displaystyle\\sum_{p_j,|p_j|=0} \\frac{r_k(p_j)}{N} \\bigg ) + \\frac{t}{N} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_page_rank(page_i):\n",
    "    sum_result = 0  \n",
    "    for page_j in pages:\n",
    "        if page_i['id'] in page_j['back_links']:\n",
    "            sum_result += page_j['rank'] / len(page_j['back_links'])\n",
    "        elif len(page_j['back_links']) == 0:\n",
    "            sum_result += page_j['rank'] / n\n",
    "    return d * sum_result + t / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    for page in pages:\n",
    "        page['rank'] = 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion termination_condition() gibt einen boolischen Wert zurück, der sagt, ob die Abbruchsbedingung erfüllt ist oder nicht. Im Falle, dass es noch keine zwei Vergleichswerte gibt, also noch kein 'rank+1'-Schlüssel existiert, wird False zurückgegebn, ansonsten wird der Rückgabewert mit folgender Formel berechnet:\n",
    "\n",
    "$$ \\sum_{p_i,i \\in 1,...,N} abs(r_{k+1}(p_i) - r_k(p_i)) <= \\delta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termination_condition():\n",
    "    delta = 0\n",
    "    for page in pages:\n",
    "        if not 'rank+1' in page:\n",
    "            return False\n",
    "        else:\n",
    "            delta += abs(page['rank+1'] - page['rank'])\n",
    "    return  delta <= δ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion set_rank() setzt für die übergebene Seite, falls der Schlüssel rank+1 existiert, den älteren PageRank-Wert auf den aktuellen Wert der Variable rank+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_rank(page):\n",
    "    if not 'rank+1' in page:\n",
    "        return\n",
    "    else:\n",
    "        page['rank'] = page['rank+1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating PageRanks for all Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden wird zunächst die Methode initialize() aufgerufen um alle Seiten mit einem initalen PageRank-Wert zu initialisieren. Anschließend wird so lange eine While-Schleife durchlaufen, bis die Abbruchsbedingung erfüllt ist. Innerhalb der While-Schleife wird jede Seite durchlaufen, wobei jeweils die Methode set_rank() aufgerufen wird und anschließend der nächste PageRank für dei Seite berechnet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initialize()\n",
    "while not termination_condition():\n",
    "    for page in pages:\n",
    "        set_rank(page)\n",
    "        page['rank+1'] = calculate_page_rank(page)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving PageRanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Resultate werden in der Datei rank.txt gespeichert. Dabei steht zunächst die id der Seite und anscließedn der eigentliche Wert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('rank.txt', 'w') as f:\n",
    "    for page in pages:\n",
    "        f.write(\"%s: %s\\n\" % (page['id'], page['rank+1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the sum of all PageRanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird zusätzlich die Summe aller PageRanks berechnet, um zu prüfen, ob sich der Wert der Zahl 1 annähert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.021723309931979\n"
     ]
    }
   ],
   "source": [
    "rank_sum = 0\n",
    "for page in pages:\n",
    "    rank_sum += page['rank+1']\n",
    "print(rank_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Build a tf-Index for the words contained in the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{t,d} =\n",
    "  \\begin{cases}\n",
    "    1 + log_{10}(tf_{t,d})       & \\quad \\text{if } tf_{t,d} > 0\\\\\n",
    "    0 & \\quad \\text{otherwise}\\\\\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_dict = {}\n",
    "term_set = set()\n",
    "stopwords = []\n",
    "exclude = set(string.punctuation)\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "with open('stop_words.txt') as line:\n",
    "    stopwords += re.sub('[^a-zA-Z0-9,]', '', line.read()).split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of unique terms in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            term_set.add(porter.stem(term).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    tf_dict[page['id']] = {}\n",
    "    for term in term_set:\n",
    "        tf_dict[page['id']][term] = 0\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            tf_dict[page['id']][porter.stem(term).lower()] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Term Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_tf(doc_id, term):\n",
    "    if term in tf_dict[doc_id]:\n",
    "        if tf_dict[doc_id][term] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 + math.log10(tf_dict[doc_id][term])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Term Frequency (TF-Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(tf_dict)\n",
    "tf_df.to_csv('index.txt', header=True, index=True, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a function search to search for documents containing given words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ idf_t = log_{10}( \\frac{N}{df_t}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ tfidf = tf * idf $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            if porter.stem(term).lower() in df_dict and page['id'] not in df_dict[porter.stem(term).lower()]['documents']:\n",
    "                df_dict[porter.stem(term).lower()]['count'] += 1\n",
    "                df_dict[porter.stem(term).lower()]['documents'] += [page['id']]\n",
    "            elif porter.stem(term).lower() not in df_dict:\n",
    "                df_dict[porter.stem(term).lower()] = {'count': 1,\n",
    "                                                      'documents': [page['id']]\n",
    "                                                     }          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Inverse Document Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_idf(term):\n",
    "    if term in df_dict:\n",
    "        if df_dict[term]['count'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.log10( n / df_dict[term]['count'])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_tf_idf(doc_id, term):\n",
    "    return get_weighted_tf(doc_id, term) * get_weighted_idf(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(terms, page_rank=False):\n",
    "    result = {}\n",
    "    for page in pages:\n",
    "        result[page['id']] = 0\n",
    "        for term in terms:\n",
    "            result[page['id']] += get_weighted_tf_idf(page['id'], porter.stem(term).lower())\n",
    "            if page_rank:\n",
    "                result[page['id']] *= page['rank+1']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms = [['token'],['index'],['classification'],['classification', 'token']]\n",
    "\n",
    "with open('tfidf_search.txt', 'w') as f:\n",
    "    for search_term in search_terms:\n",
    "        f.write('Suchwort: %s\\n\\n' % ', '.join(search_term))\n",
    "        result = search(search_term)\n",
    "        for key in result:\n",
    "            f.write(\"%s: %s\\n\" % (key, result[key]))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## f) Extend your search function and include PageRank to score the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching with TF-IDF and PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms = [['token'],['index'],['classification'],['classification', 'token']]\n",
    "\n",
    "with open('pageranke_search.txt', 'w') as f:\n",
    "    for search_term in search_terms:\n",
    "        f.write('Suchwort: %s\\n\\n' % ', '.join(search_term))\n",
    "        result = search(search_term, page_rank=True)\n",
    "        for key in result:\n",
    "            f.write(\"%s: %s\\n\" % (key, result[key]))\n",
    "        f.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
