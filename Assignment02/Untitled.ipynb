{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing crawled documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'sitespider/sites'\n",
    "files = [x[2] for x in os.walk(directory)][0]\n",
    "pages = []\n",
    "\n",
    "for file in files:\n",
    "    with open(\"%s/%s\" % (directory, file)) as json_data:\n",
    "        pages += [json.load(json_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Calculate the PageRanks of the downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(pages)\n",
    "t = 0.05\n",
    "d =  1 - t\n",
    "δ = 0.04 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PageRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_page_rank(page_i):\n",
    "    sum_result = 0  \n",
    "    for page_j in pages:\n",
    "        if page_i['id'] in page_j['back_links']:\n",
    "            sum_result += page_j['old_rank'] / len(page_j['back_links'])\n",
    "        elif len(page_j['back_links']) == 0:\n",
    "            sum_result += page_j['old_rank'] / n\n",
    "    return d * sum_result + t / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    for page in pages:\n",
    "        page['rank'] = 1/n\n",
    "\n",
    "def termination_condition():\n",
    "    dif_sum = 0\n",
    "    for page in pages:\n",
    "        if not 'old_rank' in page:\n",
    "            return False\n",
    "        else:\n",
    "            dif_sum += abs(page['rank'] - page['old_rank'])\n",
    "    return  dif_sum <= δ\n",
    "\n",
    "def set_old_ranks():\n",
    "    for page in pages:\n",
    "        page['old_rank'] = page['rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating PageRanks for all Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initialize()\n",
    "while not termination_condition():\n",
    "    set_old_ranks()\n",
    "    for page in pages:  \n",
    "        page['rank'] = calculate_page_rank(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving PageRanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('rank.txt', 'w') as f:\n",
    "    for page in pages:\n",
    "        f.write(\"%s: %s\\n\" % (page['url'], page['rank']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the sum of all PageRanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "rank_sum = 0\n",
    "for page in pages:\n",
    "    rank_sum += page['rank']\n",
    "print(rank_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Build a tf-Index for the words contained in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_dict = {}\n",
    "term_set = set()\n",
    "stopwords = []\n",
    "exclude = set(string.punctuation)\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "with open('stop_words.txt') as line:\n",
    "    stopwords += re.sub('[^a-zA-Z0-9,]', '', line.read()).split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of unique terms in documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            term_set.add(porter.stem(term).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    tf_dict[page['id']] = {}\n",
    "    for term in term_set:\n",
    "        tf_dict[page['id']][term] = 0\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            tf_dict[page['id']][porter.stem(term).lower()] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Term Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_tf(doc_id, term):\n",
    "    if term in tf_dict[doc_id]:\n",
    "        if tf_dict[doc_id][term] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 + math.log10(tf_dict[doc_id][term])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Term Frequency (TF-Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_df = pd.DataFrame(tf_dict)\n",
    "tf_df.to_csv('index.txt', header=True, index=True, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Implement a function search to search for documents containing given words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    for term in nltk.word_tokenize(page['text']):\n",
    "        if term not in exclude and term not in stopwords:\n",
    "            if porter.stem(term).lower() in df_dict and page['id'] not in df_dict[porter.stem(term).lower()]['documents']:\n",
    "                df_dict[porter.stem(term).lower()]['count'] += 1\n",
    "                df_dict[porter.stem(term).lower()]['documents'] += [page['id']]\n",
    "            elif porter.stem(term).lower() not in df_dict:\n",
    "                df_dict[porter.stem(term).lower()] = {'count': 1,\n",
    "                                                      'documents': [page['id']]\n",
    "                                                     }          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted Inverse Document Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_idf(term):\n",
    "    if term in df_dict:\n",
    "        if df_dict[term]['count'] == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.log10( n / df_dict[term]['count'])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weighted_tf_idf(doc_id, term):\n",
    "    return get_weighted_tf(doc_id, term) * get_weighted_idf(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(terms, page_rank=False):\n",
    "    result = {}\n",
    "    for page in pages:\n",
    "        result[page['id']] = 0\n",
    "        for term in terms:\n",
    "            result[page['id']] += get_weighted_tf_idf(page['id'], porter.stem(term).lower())\n",
    "            if page_rank:\n",
    "                result[page['id']] *= page['rank']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching with TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d01': 0.26556622014976977, 'd02': 0.30150996489407533, 'd03': 0.26556622014976977, 'd04': 0.30150996489407533, 'd05': 0.0, 'd06': 0.0, 'd07': 0.0, 'd08': 0.32701245764361475}\n",
      "{'d01': 0.0, 'd02': 0.0, 'd03': 0.0, 'd04': 0.6824274635301139, 'd05': 0.5541980979011975, 'd06': 0.0, 'd07': 0.0, 'd08': 0.6824274635301139}\n",
      "{'d01': 0.0, 'd02': 0.0, 'd03': 0.0, 'd04': 0.0, 'd05': 0.0, 'd06': 0.4259687322722811, 'd07': 0.4259687322722811, 'd08': 0.6824274635301139}\n",
      "{'d01': 0.26556622014976977, 'd02': 0.30150996489407533, 'd03': 0.26556622014976977, 'd04': 0.30150996489407533, 'd05': 0.0, 'd06': 0.4259687322722811, 'd07': 0.4259687322722811, 'd08': 1.0094399211737286}\n"
     ]
    }
   ],
   "source": [
    "print(search(['token']))\n",
    "print(search(['index']))\n",
    "print(search(['classification']))\n",
    "print(search(['classification', 'token']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## f) Extend your search function and include PageRank to score the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching with TF-IDF and PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d01': 0.031140856774395202, 'd02': 0.03782341969469127, 'd03': 0.03331439677287726, 'd04': 0.0674361001749996, 'd05': 0.0, 'd06': 0.0, 'd07': 0.0, 'd08': 0.0023838038549791668}\n",
      "{'d01': 0.0, 'd02': 0.0, 'd03': 0.0, 'd04': 0.15263258980165115, 'd05': 0.06498644135406392, 'd06': 0.0, 'd07': 0.0, 'd08': 0.004974652128022696}\n",
      "{'d01': 0.0, 'd02': 0.0, 'd03': 0.0, 'd04': 0.0, 'd05': 0.0, 'd06': 0.0604090817955079, 'd07': 0.0604090817955079, 'd08': 0.004974652128022696}\n",
      "{'d01': 0.031140856774395202, 'd02': 0.03782341969469127, 'd03': 0.03331439677287726, 'd04': 0.0674361001749996, 'd05': 0.0, 'd06': 0.008566960170784888, 'd07': 0.008566960170784888, 'd08': 0.0024200672897284462}\n"
     ]
    }
   ],
   "source": [
    "print(search(['token'], page_rank=True))\n",
    "print(search(['index'], page_rank=True))\n",
    "print(search(['classification'], page_rank=True))\n",
    "print(search(['classification', 'token'], page_rank=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
