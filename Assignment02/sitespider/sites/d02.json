{"id": "d02", "text": "\n\nToken normalization is the process of canonicalizing \ntokens so that matches occur despite superficial \ndifferences in the character sequences of the tokens. \n\n\n \n \n \n \n\n\n", "url": "http://people.f4.htw-berlin.de/~zhangg/pages/teaching/pages/d02.html", "back_links": ["d03", "d04", "d01", "d05"]}